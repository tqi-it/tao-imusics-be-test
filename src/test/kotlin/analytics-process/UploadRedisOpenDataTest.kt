package `analytics-process`

import com.fasterxml.jackson.module.kotlin.jacksonObjectMapper
import io.restassured.RestAssured
import org.junit.jupiter.api.Assertions.assertTrue
import io.restassured.RestAssured.given
import io.restassured.http.ContentType
import org.awaitility.Awaitility
import org.awaitility.kotlin.matches
import org.awaitility.kotlin.untilCallTo
import org.junit.jupiter.api.Assertions.*
import org.junit.jupiter.api.BeforeAll
import org.junit.jupiter.api.Disabled
import org.junit.jupiter.api.Tag
import org.junit.jupiter.api.Test
import org.junit.jupiter.api.Timeout
import org.springframework.core.annotation.MergedAnnotations.Search
import redis.clients.jedis.JedisPooled
import util.*
import util.ListsConstants.SUMMARY_RULES
import java.io.File
import java.time.format.DateTimeFormatter
import java.util.concurrent.TimeUnit
import util.RedisUtils.getRedisKeys
import util.RedisClient.jedis
import util.RedisUtils.cleanupDate
import util.Data.Companion.BASE_URL_ANALYTICS
import util.Data.Companion.DIR_SUMMARY_DUMP
import util.Data.Companion.NUMBER_OF_STREAMS
import util.ProcessStatus.aguardarProcessoCompleto
import util.RedisUtils.compararRedisComTsv
import util.RedisUtils.localizarArquivoTsv
import util.RedisUtils.validarSchemaRedisSumarizado
import util.StartProcess.PostStartProcess


class UploadRedisOpenDataTest {

    companion object {
        private var token: String = ""

        @JvmStatic
        @BeforeAll
        fun setup() {
            RestAssured.baseURI = BASE_URL_ANALYTICS
            val response = givenOauth()
            token = response.jsonPath().getString("token")
            assertNotNull(token, "Token nÃ£o deve ser nulo")
        }

    }


    /**
     * ğŸ”¥ FunÃ§Ã£o Test Redis â€” o que este teste valida
     *
     * Este teste garante que, apÃ³s a execuÃ§Ã£o completa do processo de ingestÃ£o,
     * todos os dados abertos foram corretamente enviados, estruturados e estÃ£o
     * consistentes entre os arquivos .tsv gerados e os dados armazenados no Redis.
     *
     * âœ” Fluxo validado pelo teste:
     *
     * 1ï¸âƒ£ Dispara o processo de ingestÃ£o via /start-process
     *     - Envia a data desejada
     *     - Valida que o processo iniciou com sucesso
     *
     * 2ï¸âƒ£ Aguarda a conclusÃ£o do processo
     *     - Verifica periodicamente /process-status usando Awaitility
     *     - SÃ³ avanÃ§a quando status = "completed"
     *
     * 3ï¸âƒ£ Valida a entrega de dados ao Redis
     *     - Lista todas as chaves com padrÃ£o:
     *           imusic:*:<date>:*
     *     - Garante que pelo menos uma chave foi criada
     *
     * 4ï¸âƒ£ Valida estrutura de cada chave encontrada no Redis
     *     - Para HASH:
     *         âœ” Deve existir
     *         âœ” NÃ£o pode estar vazia
     *     - Para LIST:
     *         âœ” Deve existir
     *         âœ” Deve ter elementos (>0)
     *         âœ” Carrega amostras (atÃ© 3 itens) e imprime no log
     *
     * 5ï¸âƒ£ Localiza o arquivo .tsv correspondente no diretÃ³rio /tmp
     *     - Converte informaÃ§Ãµes da chave Redis (platform/date)
     *     - Encontra o arquivo real com matching (ex: iMusics_Amazon_2025-11-15.tsv)
     *
     * 6ï¸âƒ£ Compara dados abertos:
     *     - Carrega o arquivo TSV linha a linha
     *     - Carrega a lista correspondente no Redis
     *     - Compara:
     *         âœ” quantidade de registros
     *         âœ” conteÃºdo de cada linha
     *     - Exporta JSON temporÃ¡rio para facilitar debug em caso de falha
     *
     * 7ï¸âƒ£ Valida integridade total dos dados
     *     - Qualquer divergÃªncia de conteÃºdo â†’ falha o teste
     *     - Qualquer chave inesperada (string, set, zset) â†’ falha
     *     - Qualquer chave vazia â†’ falha
     *
     * âœ” Este teste certifica que:
     *     - O processo executou sem erro
     *     - As chaves esperadas foram criadas corretamente
     *     - Os dados abertos estÃ£o consistentes entre Redis e TSV
     *     - NÃ£o existem chaves vazias ou tipos incorretos
     *     - O pipeline de ingestÃ£o gera arquivos vÃ¡lidos e os dados publicados no
     *       Redis correspondem exatamente ao conteÃºdo processado
     *
     * â• BenefÃ­cios:
     *     - Garante integridade ponta-a-ponta
     *     - Garante que Redis nÃ£o recebeu dados duplicados, vazios ou corrompidos
     *     - Detecta divergÃªncias nos pipelines de ETL
     *     - Serve como teste de regressÃ£o completo do processo de ingestÃ£o
     */

    @Test
    @Tag("smokeTests") // TPF-70
    @Timeout(value = 45, unit = TimeUnit.MINUTES)
    fun `CN8 - Validar entrega dos dados abertos no Redis 'process_file_to_redis'`() {

        val formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd")
        //val date = LocalDate.now().plusDays(-2).format(formatter)
        var startDate ="2025-11-03"
        var endDate ="2025-11-04"

        LogCollector.println("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        LogCollector.println("ğŸ§ª CN8 - Validar entrega dos dados abertos no Redis 'process_file_to_redis'")
        LogCollector.println("ğŸ“… Data utilizada: $startDate e $endDate")
        LogCollector.println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

        LogCollector.println("ğŸš€ PASSO 1: Startando processamento dos perÃ­odos: $startDate | $endDate ...")
        val response = PostStartProcess(
            startDate = startDate,
            endDate = endDate,
            token = token)
        assertTrue(response?.extract()?.statusCode() == 200)
        assertEquals("Process started (background)", response?.extract()?.jsonPath()?.getString("message"))


        LogCollector.println("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        LogCollector.println("ğŸš€ PASSO 2: Aguardando conclusÃ£o do processamento...")
        Awaitility.await()
            .atMost(90, TimeUnit.MINUTES)
            .pollInterval(5, TimeUnit.MINUTES)
            .ignoreExceptions()
            .until {
                val resp = given()
                    .header("authorization", "Bearer $token")
                    .header("origin", "http://localhost")
                    .get("/process-status")
                    .then()
                    .extract()

                val status = resp.jsonPath().getString("status") ?: ""
                val msg = resp.jsonPath().getString("message") ?: ""

                LogCollector.println("ğŸ”„ Status â†’ $status | msg: $msg")
                status.equals("completed", ignoreCase = true)
            }


        LogCollector.println("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        LogCollector.println("ğŸš€ PASSO 3: Validando Redis\n")
        val keys = getRedisKeys("imusic:*:$startDate:*")
        assertTrue(keys.isNotEmpty(), "Nenhuma chave encontrada no Redis para $startDate")

        LogCollector.println("ğŸ“Œ Chaves encontradas:")
        keys.forEach { LogCollector.println(" â†’ $it") }

        // ============================================================================
        //   ğŸ”¥ NOVA LÃ“GICA â€” GRUPO POR PLATAFORMA E VALIDAR TUDO
        // ============================================================================

        val players = keys
            .map { it.split(":")[2] }
            .distinct()

        players.forEach { player ->
            val metaKey = keys.firstOrNull { it.contains(":$player:") && it.endsWith(":meta") }
            val rowsKey = keys.firstOrNull { it.contains(":$player:") && it.endsWith(":rows") }

            if (metaKey == null || rowsKey == null) {
                LogCollector.println("â„¹ Ignorando player '$player' â€” nÃ£o possui meta/rows completos!")
                return@forEach
            }

            LogCollector.println("\n============================================================")
            LogCollector.println("ğŸ§ VALIDANDO PLAYER: $player")
            LogCollector.println("============================================================")

            LogCollector.println("META â†’ $metaKey")
            LogCollector.println("ROWS â†’ $rowsKey\n")

            RedisUtils.validarSchemaRedis(metaKey, "hash")
            RedisUtils.validarRowCountConsistente(metaKey, rowsKey)
            RedisUtils.validarSchemaRedis(rowsKey, "list")

            // apenas se a lista nÃ£o Ã© de agregaÃ§Ã£o
            if (!player.contains("topalbuns") && !player.contains("topplaysremunerado")) {
                val tsvFile = localizarArquivoTsv(rowsKey)
                compararRedisComTsv(rowsKey, tsvFile)
            }

            RedisUtils.printRedisKeyContentToFile(metaKey)
            RedisUtils.printRedisKeyContentToFile(rowsKey)
        }


        // FinalizaÃ§Ã£o
        LogCollector.println("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        LogCollector.println("ğŸš€ PASSO 6: Validando status final do processamento\n")
        ProcessStatus.aguardarProcessoCompleto(token = token)
        LogCollector.println("\nâœ” ExecuÃ§Ã£o finalizada com sucesso garantindo ate a etapa 'Finalizado'.\n")
    }

    @Test
    @Tag("smokeTests") // TPF-68
    @Timeout(value = 45, unit = TimeUnit.MINUTES)
    fun `CN9 - Validar entrega dos dados abertosXagrupados no Redis 'sumarize_tops'`() {

        val formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd")
        val date = "2025-11-12"//LocalDate.now().plusDays(-60).format(formatter)

        LogCollector.println("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        LogCollector.println("ğŸ§ª CN9 - Validar entrega dos dados abertosXagrupados no Redis 'sumarize_tops'")
        LogCollector.println("ğŸ“… Data utilizada: $date")
        LogCollector.println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")

        // ğŸ”¥ Limpa o Redis ANTES de iniciar para melhorar a performance do teste
        cleanupDate(date)
        println("Redis apÃ³s cleanup:")
        getRedisKeys("imusic:*:$date:*").forEach { println(" - $it") }
        println("ANTES DE INICIAR O PROCESSO:")
        jedis.lrange("imusic:topplaysremunerado:$date:rows", 0, 5)
            .forEach { println(it) }


        val startResponse = PostStartProcess (startDate = date, endDate = date, token = token)
        startResponse?.extract()?.jsonPath()?.getBoolean("success")?.let { assertTrue(it) }

        LogCollector.println("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        LogCollector.println("ğŸš€ PASSO 1: Processo iniciado...")

        // Aguarda liberar Redis
        aguardarProcessoCompleto(token = token)

        LogCollector.println("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        LogCollector.println("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        LogCollector.println("ğŸ•µï¸â€â™‚ PASSO 2: Validando sumarizaÃ§Ãµes TOP no Redis...")

        val plataformas = detectarPlataformas(date)
        assertTrue(plataformas.isNotEmpty(), "Nenhuma plataforma encontrada para $date!")
        LogCollector.println("ğŸ“Œ Plataformas detectadas: $plataformas")


        // 1) Carregar dados brutos por plataforma (imusic:*:<PLATAFORMA>:<DATA>:rows)
        val rawRowsByPlatform = plataformas.associateWith { plataforma ->
            //val keyRows = getRedisKeys("imusic:*:${plataforma}:${date}:rows").first()
            val keyRows = getRedisKeys("imusic:dashes:${plataforma}:${date}:rows")
                .firstOrNull()
                ?: error("âŒ Nenhum RAW encontrado em dashes para $plataforma")

            // 1ï¸âƒ£ Carrega TUDO em lista (seguro)
            println("Cheguei aqui no loadRawRowsWindowed")
            // ğŸš¨ DEBUG: limita a quantidade de registros para testar sem travar 50 mil passou
            //val rows: List<Map<String, Any?>> = rawSeq.take(100_000).toList()

            val rows = loadRawRowsWindowed(jedis, keyRows, plataforma)
            //val rows = loadRawRows(jedis, keyRows, plataforma)
            rows

        }

        // 3) Validar cada sumarizaÃ§Ã£o para cada plataforma
        plataformas.forEach { plataforma ->
            // 3.1) Definir regras de sumarizaÃ§Ã£o
            SUMMARY_RULES.forEach { (prefix, campo, metric) ->
                val summaryKey =
                    when (prefix) {
                        "topalbuns" ->
                            "imusic:topalbuns:${date}:rows"

                        "topplaysremunerado" ->
                            "imusic:topplaysremunerado:${date}:rows"

                        else ->
                            "imusic:${prefix}:${plataforma}:${date}:rows"
                    }

                validarSchemaRedisSumarizado(summaryKey)

                LogCollector.println("\nğŸ” Validando sumarizaÃ§Ã£o â†’ $summaryKey")
                validarSumarizacao(
                    summaryKey,
                    rawRowsByPlatform[plataforma]!!,
                    campo,
                    DIR_SUMMARY_DUMP
                )
            }
        }

        LogCollector.println("\nğŸ‰ Todas as sumarizaÃ§Ãµes validadas com sucesso!")
        LogCollector.println("âœ” ExecuÃ§Ã£o finalizada com sucesso garantindo ate a etapa 'sumarize_tops'.\n")
    }

    @Test
    @Tag("test")
    @Disabled("Somente Testes de consulta no Redis")
    fun SearchRedis(){
        val rawDirect = jedis.lrange("imusic:topplaysremunerado:2025-11-11:rows", 0, 10)
        rawDirect.forEach { println("DIRECT RAW = $it") }

        rawDirect.take(20).forEach { row ->
            println("REDIS PARSED => $row")
        }
    }

    /**
     *FunÃ§Ã£o para Paginar de 50 em 50 mil linhas os dados do Redis
     * â†’ Carrega TUDO em memÃ³ria retornando List completa na memÃ³ria
     */
    fun readLargeRedisListPaged(
        jedis: JedisPooled,
        key: String,
        pageSize: Int = 50000
    ): List<String> {

        println("Iniciando paginaÃ§Ã£o Redis para key=$key pageSize=$pageSize")

        val result = mutableListOf<String>()

        var start = 0L
        val step = pageSize.toLong()

        while (true) {
            val end = start + step - 1

            println("â¡ï¸  Lendo pÃ¡gina: start=$start end=$end")

            val page = jedis.lrange(key, start, end)

            if (page.isEmpty()) {
                println("âœ… page vazia -> fim da paginaÃ§Ã£o")
                break
            }

            result.addAll(page)
            start += step
        }

        println("ğŸ PaginaÃ§Ã£o FINALIZADA")
        return result
    }

    /**
     *FunÃ§Ã£o para Carregar de 50 em 50 mil linhas os dados do Redis
     * â†’ Stream LAZY (Sequence)
         * NÃ£o acumula nada na memÃ³ria
         * Itera pageSize por vez (ex: 50.000 itens)
         * Consegue processar milhÃµes de registros sem travar
         * JÃ¡ parseia JSON â†’ Map corretamente a cada elemento
     */
    fun loadRawRowsWindowed(
        jedis: JedisPooled,
        key: String,
        plataforma: String,
        pageSize: Int = 25_000
    ): Sequence<Map<String, Any?>> = sequence {

        var start = 0L
        val step = pageSize.toLong()

        while (true) {
            val end = start + step - 1

            println("â¡ï¸  Lendo janela: start=$start end=$end")

            val page = jedis.lrange(key, start, end)
            if (page.isEmpty()) {
                println("âœ… page vazia -> fim")
                break
            }

            for (json in page) {
                val map = RedisUtils.jsonToMap(json).toMutableMap()
                map["plataform"] = plataforma
                yield(map)
            }

            start += step
        }
        println("ğŸ Carregamento de PaginaÃ§Ã£o FINALIZADA")
    }

    /**
     *FunÃ§Ã£o para identificar as Plataformas a serem processadas
     */
    fun detectarPlataformas(date: String): Set<String> {
        val keys = getRedisKeys("imusic:*:*:$date:rows")

        return keys.mapNotNull { key ->
            val partes = key.split(":")
            if (partes.size >= 5) partes[2] else null
        }.toSet()
    }

    /**
    ğŸ§  Objetivo: A funÃ§Ã£o valida se a sumarizaÃ§Ã£o gravada no Redis estÃ¡ correta, comparando:
        - o que estÃ¡ no Redis
        - com o que deveria estar, calculado novamente no teste (ground truth)
    Ela garante que a lÃ³gica real de sumarizaÃ§Ã£o do pipeline estÃ¡ funcionando exatamente como foi especificado.

    A validaÃ§Ã£o ocorre em 3 dimensÃµes:
        - As chaves agrupadas sÃ£o as mesmas
        - A quantidade de grupos Ã© igual (tamanho da sumarizaÃ§Ã£o)
        - O valor somado (number_of_streams) por grupo Ã© igual
     */
    fun validarSumarizacao(
        summaryKey: String,
        rawRows: Sequence<Map<String, Any?>>,
        campo: String,
        dumpDir: String
    ) {
        val jedis = RedisClient.jedis

        LogCollector.println("ğŸ“Œ Iniciando validaÃ§Ã£o detalhada â†’ $summaryKey")

        // 1. PaginaÃ§Ã£o Redis
        val redisSummary = readLargeRedisListPaged(jedis, summaryKey)
            .map { RedisUtils.jsonToMap(it) }

        // 2. Config do agrupamento
        val config = getSumarizacaoConfig(summaryKey, campo)

        // 3. Recalcula sumarizaÃ§Ã£o
        val expected = recalcularSumarizacao(rawRows, config)

        // 4. Converte Redis para Map (mesmo padrÃ£o do Python)
        //println("### RAW redisSummary (first) = ${redisSummary.first()}")
        val redisMap = redisSummary.associate { row ->

            // Normaliza TODAS as chaves vindas do Redis para lowercase
            val normalized = row.mapKeys { (k, _) -> k.lowercase() }

            // Tratamento do "date" conforme o Python
            val dateVal = (normalized["date"]?.toString()?.trim()
                ?: config.dateFixed
                ?: "")

            // Monta a chave exatamente como no Python
            val key = config.groupFields.joinToString("|") { field ->
                when (field) {
                    "date" -> dateVal ?: ""
                    else -> row[field]?.toString()?.trim() ?: ""
                }
            }

            val streams = normalized[NUMBER_OF_STREAMS.lowercase()]?.toString()?.toIntOrNull() ?: 0
            key to streams
        }


        // 5. Dumps
        //println("### BEFORE SAVE: redisMap[1003715472151|2025-11-11] = ${redisMap["1003715472151|2025-11-11"]}")
        saveJsonToFile(dumpDir, "${summaryKey}_expected.json", expected)
        saveJsonToFile(dumpDir, "${summaryKey}_from_redis.json", redisMap)

        // 6. ValidaÃ§Ã£o de quantidade
        LogCollector.println("â¡ expected: ${expected.size}, redis: ${redisMap.size}")
        assertEquals(expected.size, redisMap.size)

        // ---> Chamada do relatÃ³rio HTML aqui <---
        GenerateHtmlReportFromDumps().report(
            summaryKey = summaryKey,
            campo = campo
        )

        // 7. ComparaÃ§Ã£o lado a lado
        LogCollector.println("ğŸ“Œ DiferenÃ§as detectadas (se houver):")

        var diffs = 0

        expected.forEach { (key, expectedStreams) ->
            val redisStreams = redisMap[key]

            if (expectedStreams != redisStreams) {
                diffs++
                LogCollector.println(
                    """
                ---
                âŒ DivergÃªncia encontrada
                key: $key
                expected_streams: $expectedStreams
                redis_streams   : $redisStreams
                ---
                """.trimIndent()
                )
            }
        }

        if (diffs == 0) {
            LogCollector.println("âœ” Nenhuma divergÃªncia encontrada")
        } else {
            LogCollector.println("âš  Total de divergÃªncias: $diffs")
        }

        assertEquals(0, diffs, "Foram encontradas divergÃªncias na sumarizaÃ§Ã£o")
    }


    /**
     * Objetivo: Recalcular por conta prÃ³pria o agrupamento verdadeiro
        - Agrupar: por um ou mais campos (definidos pela configuraÃ§Ã£o da sumarizaÃ§Ã£o)
        - Somar number_of_streams: dos registros brutos (rawRows) dentro de cada grupo
        - Gerar um mapa: onde a chave Ã© o agrupamento e o valor Ã© o total somado.

       Como o calculo Ã© feito:
        - ğŸ“‚ 1. Entrada: rawRows (dados brutos)
        - âš™ï¸ 2. ConfiguraÃ§Ã£o de sumarizaÃ§Ã£o (getSumarizacaoConfig)
                Essa funÃ§Ã£o analisa a key do Redis e devolve:
                    quais campos devem ser agrupados
                    qual campo serÃ¡ somado
        - â• 3. Agrupamento e soma (o cÃ¡lculo correto)
                PASSO 1 â€” Criar chave de agrupamento para cada row
                PASSO 2 â€” Somar os valores para cada registro
                        pega number_of_streams
                        adiciona ao total do grupo correspondente
        - ğŸ 4. Resultado final (expected)
                    a key Ã© o agrupamento
                    o value Ã© o total somado

    Agrupar os dados brutos por um ou mais campos e somar number_of_streams dentro de cada grupo.

     */
    fun recalcularSumarizacao(
        raw: Sequence<Map<String, Any?>>,
        config: SumarizacaoConfig
    ): Map<String, Int> {

        val acumulado = mutableMapOf<String, Int>()

        raw.forEach { row ->

            // --- 1. Filtragem igual ao Python ---
            for (required in config.requiredFields) {
                val valor = row[required.lowercase()]?.toString()?.trim()
                if (valor.isNullOrEmpty()) {
                    return@forEach // pula como continue
                }
            }

            // --- 2. Corrige comportamento do "date" igual ao Python ---
            var dateVal = row["date"]?.toString()?.trim()
            if (dateVal.isNullOrEmpty()) {
                // Se nÃ£o existir no JSON, usar o date vindo da prÃ³pria key
                dateVal = config.dateFixed // vocÃª deve incluir isso na config
            }

            // --- 3. Monta a chave de agrupamento exatamente igual ao Python ---
            val key = config.groupFields.joinToString("|") { field ->
                when (field) {
                    "date" -> dateVal ?: ""
                    else -> row[field]?.toString()?.trim() ?: ""
                }
            }

            // --- 4. Soma streams ---
            val streams = row[NUMBER_OF_STREAMS]?.toString()?.toIntOrNull() ?: 0

            acumulado[key] = acumulado.getOrDefault(key, 0) + streams
        }

        return acumulado
    }
    data class SumarizacaoConfig(
        val groupFields: List<String>,
        val requiredFields: List<String> = emptyList(),
        val dateFixed: String? = null
    )

    /**
     *FunÃ§Ã£o configuraÃ§Ã£o usada na sumarizaÃ§Ã£o
     */
    fun getSumarizacaoConfig(key: String, campo: String): SumarizacaoConfig {
        val k = key.lowercase()

        return when {

            // ğŸ”¥ Sempre coloque os mais especÃ­ficos primeiro
            k.contains("topplaysremunerado") && !k.contains("topregioes") ->
                SumarizacaoConfig(
                    groupFields = listOf("asset_id", "territory", "date"),
                    requiredFields = listOf("asset_id", "territory")
                )

            k.contains("topregioes") ->
                SumarizacaoConfig(
                    groupFields = listOf("asset_id", "territory", "plataform", "date"),
                    requiredFields = listOf("asset_id", "territory")
                )

            // ğŸ”¥ genÃ©rico sÃ³ depois dos especÃ­ficos
            k.contains("topplays") ->
                SumarizacaoConfig(
                    groupFields = listOf("asset_id", "date"),
                    requiredFields = listOf("asset_id")
                )

            k.contains("topalbum") ->
                SumarizacaoConfig(
                    groupFields = listOf("upc", "plataform", "date"),
                    requiredFields = listOf("upc")
                )

            k.contains("topalbuns") ->
                SumarizacaoConfig(
                    groupFields = listOf("upc", "date"),
                    requiredFields = listOf("upc")
                )

            k.contains("topplaylist") ->
                SumarizacaoConfig(
                    groupFields = listOf("asset_id", "plataform", "stream_source", "stream_source_uri", "date"),
                    requiredFields = listOf("asset_id")
                )

            k.contains("topplataform") ->
                SumarizacaoConfig(
                    groupFields = listOf("asset_id", "plataform", "date"),
                    requiredFields = listOf("asset_id")
                )

            else -> error("Tipo desconhecido: $key")
        }
    }

    /**
     *FunÃ§Ã£o para geraÃ§Ã£o e interpretaÃ§Ã£o do JSON
     */
    fun saveJsonToFile(dir: String, fileName: String, data: Any) {
        val folder = File(dir)
        if (!folder.exists()) folder.mkdirs()
        val mapper = jacksonObjectMapper()
        File(folder, fileName).writeText(mapper.writerWithDefaultPrettyPrinter().writeValueAsString(data))
    }





}